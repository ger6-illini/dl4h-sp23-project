{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c36b4ee",
   "metadata": {},
   "source": [
    "# Testing Baseline (Multitask Learning Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699e4d24",
   "metadata": {},
   "source": [
    "Importing the functions needed from the `mtl_patients` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "pathname = \"../code/\"\n",
    "if pathname not in sys.path:\n",
    "    sys.path.append(\"../code/\")\n",
    "\n",
    "from mtl_patients import get_summaries, run_mortality_prediction_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba4d09",
   "metadata": {},
   "source": [
    "Run summaries. Default (no parameters) assumes collection of data for first 24 hours and 12 hours of gap after that period to start predicting mortality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_summ_by_cu_df, pat_summ_by_sapsiiq_df, vitals_labs_summ_df = get_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64736797",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_summ_by_cu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4043c6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_summ_by_sapsiiq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ffb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_labs_summ_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44543914",
   "metadata": {},
   "source": [
    "Run the mortality prediction task using the global model. Default (no parameters) assumes collection of data for first 24 hours and 12 hours of gap after that period to start predicting mortality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3387145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 9999\n",
    "metrics_df = run_mortality_prediction_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c756eddc",
   "metadata": {},
   "source": [
    "First run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1498b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9322814",
   "metadata": {},
   "source": [
    "Second run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9939505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743a122",
   "metadata": {},
   "source": [
    "They are the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f6ebe",
   "metadata": {},
   "source": [
    "## `run_mortality_prediction()` step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea7a43",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6098381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense, LSTM, RepeatVector\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, roc_curve\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0e2f76",
   "metadata": {},
   "source": [
    "Arguments for `run_mortality_prediction()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a9fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type='multitask'\n",
    "cutoff_hours=24\n",
    "gap_hours=12\n",
    "save_to_folder='../data/'\n",
    "cohort_criteria_to_select='careunits'\n",
    "seed=0\n",
    "cohort_unsupervised_filename='../data/unsupervised_clusters.npy'\n",
    "lstm_layer_size=16\n",
    "epochs=30\n",
    "learning_rate=0.0001\n",
    "use_cohort_inv_freq_weights=False\n",
    "bootstrap=False\n",
    "num_bootstrapped_samples=100\n",
    "sensitivity=0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f03385b",
   "metadata": {},
   "source": [
    "Imports for local functions needed by `run_mortality_prediction()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2818a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtl_patients import set_global_determinism, prepare_data, stratified_split\n",
    "from mtl_patients import create_single_task_learning_model, create_multitask_learning_model\n",
    "from mtl_patients import bootstrap_predict\n",
    "from mtl_patients import get_mtl_sample_weights, get_correct_task_mtl_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143bf8ee",
   "metadata": {},
   "source": [
    "Code in `run_mortality_prediction()` common to all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3752514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the seeds to get reproducible results\n",
    "# taken from https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed\n",
    "set_global_determinism(seed=seed)\n",
    "\n",
    "# create folders to store models and results\n",
    "for folder in ['results', 'models']:\n",
    "    if not os.path.exists(os.path.join(save_to_folder, folder)):\n",
    "        os.makedirs(os.path.join(save_to_folder, folder))\n",
    "\n",
    "X, Y, careunits, sapsii_quartile, subject_ids = prepare_data(cutoff_hours=cutoff_hours, gap_hours=gap_hours)\n",
    "Y = Y.astype(int) # Y is originally a boolean\n",
    "\n",
    "print('+' * 80, flush=True)\n",
    "print('Running the Mortality Prediction Task', flush=True)\n",
    "print('-' * 80, flush=True)\n",
    "\n",
    "# fetch right cohort criteria\n",
    "if cohort_criteria_to_select == 'careunits':\n",
    "    cohort_criteria = careunits\n",
    "elif cohort_criteria_to_select == 'sapsii_quartile':\n",
    "    cohort_criteria = sapsii_quartile\n",
    "elif cohort_criteria_to_select == 'unsupervised':\n",
    "    cohort_criteria = np.load(f\"{cohort_unsupervised_filename}\")\n",
    "\n",
    "# Do train/validation/test split using `cohort_criteria` as the cohort classifier\n",
    "print('    Splitting data into train/validation/test sets...', flush=True)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, cohorts_train, cohorts_val, cohorts_test = \\\n",
    "    stratified_split(X, Y, cohort_criteria, train_val_random_seed=seed)\n",
    "\n",
    "# one task by distinct cohort\n",
    "tasks = np.unique(cohorts_train)\n",
    "\n",
    "# calculate number of samples per cohort and its reciprocal\n",
    "# (to be used in sample weight calculation)\n",
    "print('    Calculating number of training samples in cohort...', flush=True)\n",
    "task_weights = {}\n",
    "for cohort in tasks:\n",
    "    num_samples_in_cohort = len(np.where(cohorts_train == cohort)[0])\n",
    "    print(f\"        # of patients in cohort {cohort} is {str(num_samples_in_cohort)}\")\n",
    "    task_weights[cohort] = len(X_train) / num_samples_in_cohort\n",
    "\n",
    "sample_weight = None\n",
    "if use_cohort_inv_freq_weights:\n",
    "    # calculate sample weight as the cohort's inverse frequency corresponding to each sample\n",
    "    sample_weight = np.array([task_weights[cohort] for cohort in cohorts_train])\n",
    "\n",
    "model_filename = f\"{save_to_folder}models/model_{model_type}_{cutoff_hours}+{gap_hours}_{cohort_criteria_to_select}\"\n",
    "filename_part_bootstrap = \"bootstrap-ON\" if bootstrap else \"bootstrap-OFF\"\n",
    "results_filename = f'{save_to_folder}results/model_{model_type}_{cutoff_hours}+{gap_hours}'\n",
    "results_filename = results_filename + f'_{cohort_criteria_to_select}_{filename_part_bootstrap}.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47477061",
   "metadata": {},
   "source": [
    "Code specific for multitask learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37680498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------\n",
    "# train the multitask model\n",
    "\n",
    "print('    ' + '~' * 76)\n",
    "print(f\"    Training '{model_type}' model...\")\n",
    "\n",
    "num_tasks = len(tasks)\n",
    "cohort_to_index = dict(zip(tasks, range(num_tasks)))\n",
    "model = create_multitask_learning_model(lstm_layer_size=lstm_layer_size, input_dims=X_train.shape[1:],\n",
    "                                        output_dims=1, tasks=tasks, learning_rate=learning_rate)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85fe4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "\n",
    "model.fit(X_train, [y_train for i in range(num_tasks)], epochs=epochs, batch_size=100,\n",
    "        sample_weight=get_mtl_sample_weights(y_train, cohorts_train, tasks, sample_weight=sample_weight),\n",
    "        callbacks=[early_stopping],\n",
    "        validation_data=(X_val, [y_val for i in range(num_tasks)]))\n",
    "model.save(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea7b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('    ' + '~' * 76)\n",
    "print(f\"    Predicting using '{model_type}' model...\", flush=True)\n",
    "y_scores = np.squeeze(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf22455",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea250670",
   "metadata": {},
   "source": [
    "## With no bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff47dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_str = [str(task) for task in tasks]\n",
    "metrics_df = pd.DataFrame(index=np.append(tasks_str, ['Macro', 'Micro']), dtype=float)\n",
    "\n",
    "for task in tasks:\n",
    "    y_scores_in_cohort = y_scores[cohort_to_index[task], cohorts_test == task]\n",
    "    y_true_in_cohort = y_test[cohorts_test == task]\n",
    "\n",
    "    ## get TPR, aka sensitivity, and thresholds (using micro metric)\n",
    "    _, tpr, thresholds = roc_curve(y_true_in_cohort, y_scores_in_cohort)\n",
    "    ## threshold close to give target TPR, e.g., 80%\n",
    "    threshold_target = thresholds[np.argmin(np.abs(tpr - sensitivity))]\n",
    "    ### Why 80% threshold? That is what the paper selected to display the results\n",
    "    ## use calculated threshold to do predictions\n",
    "    y_pred_in_cohort = (y_scores_in_cohort > threshold_target).astype(\"int32\")\n",
    "\n",
    "    auc = roc_auc_score(y_true_in_cohort, y_scores_in_cohort)\n",
    "    ppv = precision_score(y_true_in_cohort, y_pred_in_cohort, zero_division=0)\n",
    "    specificity = recall_score(y_true_in_cohort, y_pred_in_cohort, pos_label=0)\n",
    "    metrics_df.loc[str(task), 'AUC'] = auc\n",
    "    metrics_df.loc[str(task), 'PPV'] = ppv\n",
    "    metrics_df.loc[str(task), 'Specificity'] = specificity\n",
    "\n",
    "# calculate macro metrics\n",
    "metrics_df.loc['Macro', :] = metrics_df.loc[(metrics_df.index != 'Macro') & (metrics_df.index != 'Micro')].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a369a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e5f145-25a3-44d0-96e7-d316833ffe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, tpr, thresholds = roc_curve(y_test, y_scores[[cohort_to_index[c] for c in cohorts_test], np.arange(len(y_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b5e95-74b9-4ae4-aeef-6c5788b0a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_target = thresholds[np.argmin(np.abs(tpr - sensitivity))]\n",
    "threshold_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0755957-10af-4076-bf15-21638f706a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_scores[[cohort_to_index[c] for c in cohorts_test], np.arange(len(y_test))] > threshold_target).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02140e7f-ddc7-448e-a7fa-9fec2ee98acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a1ebf9-c5ea-4456-8ee0-5bcc235c10a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.loc['Micro', 'PPV'] = precision_score(y_test, y_pred)#[[cohort_to_index[c] for c in cohorts_test], np.arange(len(y_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacda247-60ee-41f6-b0cd-d3db338b9269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad637d69-e0aa-4b96-9d53-7a4150fef85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate micro metrics\n",
    "## get TPR, aka sensitivity, and thresholds\n",
    "_, tpr, thresholds = roc_curve(y_test, y_scores[[cohort_to_index[c] for c in cohorts_test], np.arange(len(y_test))])\n",
    "## threshold close to give target TPR, e.g., 80%\n",
    "threshold_target = thresholds[np.argmin(np.abs(tpr - sensitivity))]\n",
    "### Why 80% threshold? That is what the paper selected to display the results\n",
    "## use calculated threshold to do predictions\n",
    "y_pred = (y_scores[[cohort_to_index[c] for c in cohorts_test], np.arange(len(y_test))] > threshold_target).astype(\"int32\")\n",
    "\n",
    "metrics_df.loc['Micro', 'AUC'] = roc_auc_score(y_test, y_scores[[cohort_to_index[c] for c in cohorts_test], np.arange(len(y_test))])\n",
    "metrics_df.loc['Micro', 'PPV'] = precision_score(y_test, y_pred[[cohort_to_index[c] for c in cohorts_test], np.arange(len(y_test))])\n",
    "metrics_df.loc['Micro', 'Specificity'] = recall_score(y_test, y_pred[[cohort_to_index[c] for c in cohorts_test], np.arange(len(y_test))], pos_label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8059845b",
   "metadata": {},
   "source": [
    "## With bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get `num_bootstrapped_samples` and calculate AUC, PPV, and specificity\n",
    "\n",
    "tasks_str = [str(task) for task in tasks]\n",
    "lst_of_tasks = list(tasks_str)\n",
    "lst_of_tasks.append('Micro')\n",
    "lst_of_tasks.append('Macro')\n",
    "\n",
    "idx = pd.MultiIndex.from_product([lst_of_tasks, list(np.arange(1, 101).astype(str))], names=['Cohort', 'Sample'])\n",
    "metrics_df = pd.DataFrame(index=idx, columns=['AUC', 'PPV', 'Specificity'], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae88fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tasks:\n",
    "    all_auc, all_ppv, all_specificity = bootstrap_predict(X_test, y_test, cohorts_test, task, model,\n",
    "                                                          tasks=tasks, num_bootstrap_samples=num_bootstrapped_samples)\n",
    "    metrics_df.loc[str(task), 'AUC'] = all_auc\n",
    "    metrics_df.loc[str(task), 'PPV'] = all_ppv\n",
    "    metrics_df.loc[str(task), 'Specificity'] = all_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f5154",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f01da-c1ae-4094-b487-a7cdefde9a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.query(\"Cohort != 'Micro' and Cohort != 'Macro'\").groupby('Sample').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.query(\"Cohort != 'Micro' and Cohort != 'Macro'\").groupby('Sample').mean()['AUC'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa77ab7c-53d2-4e79-aaea-ecdd14ec674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.loc['Macro', 'AUC'] = metrics_df.query(\"Cohort != 'Micro' and Cohort != 'Macro'\").groupby('Sample').mean()['AUC'].values\n",
    "metrics_df.loc['Macro', 'PPV'] = metrics_df.query(\"Cohort != 'Micro' and Cohort != 'Macro'\").groupby('Sample').mean()['PPV'].values\n",
    "metrics_df.loc['Macro', 'Specificity'] = metrics_df.query(\"Cohort != 'Micro' and Cohort != 'Macro'\").groupby('Sample').mean()['Specificity'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d4c008-ac54-4911-92e0-bd67c9c84dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.loc['Macro', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2ad9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_auc, all_ppv, all_specificity = bootstrap_predict(X_test, y_test, cohorts_test, 'all', model,\n",
    "                                                      tasks=tasks, num_bootstrap_samples=num_bootstrapped_samples)\n",
    "metrics_df.loc['Micro', 'AUC'] = all_auc\n",
    "metrics_df.loc['Micro', 'PPV'] = all_ppv\n",
    "metrics_df.loc['Micro', 'Specificity'] = all_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc79be",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec5141",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
