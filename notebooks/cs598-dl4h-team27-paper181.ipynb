{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652e22f4",
   "metadata": {},
   "source": [
    "<h1><center>CS598 Deep Learning for Healthcare Spring 2023<br>Paper Reproduction Project</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee5cbba",
   "metadata": {},
   "source": [
    "<h3><center>Gilberto Ramirez and Jay Kawkani<br><span style=\"font-family:monospace;\">{ger6, kakwani2}@illinois.edu</span><br><font color=\"lightgrey\">Group ID: 27 | Paper ID: 181</font></center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612cc966",
   "metadata": {},
   "source": [
    "In this project, we aim to reproduce the paper [*Learning Task for Multitask Learning: Heterogeneous Patient Populations in the ICU* by (Suresh et al, 2018)](https://arxiv.org/abs/1806.02878). In this paper, the authors propose a novel two-step pipeline to predict in-hospital mortality across patient populations with different characteristics. The first step of the pipeline divides patients into relevant non-overlapping cohorts in an unsupervised way using a long short-term memory (LSTM) autoencoder followed by a Gaussian Mixture Model (GMM). The second step of the pipeline predicts in-hospital mortality for each patient cohort identified in the previous step using an LSTM based multi-task learning model where every cohort is considered a different task.\n",
    "The paper claims that by applying this pipeline, the multi-task learning model can leverage shared knowledge across the distinct patient groups identified and it can work effectively since the groups were obtained using a data-driven method rather than relying in domain knowledge or auxiliary labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f018d",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb3581e",
   "metadata": {},
   "source": [
    "1. [Data](#section-1)\n",
    "2. [Methods](#section-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e318a",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"section-1\">1. Data</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb58470c",
   "metadata": {},
   "source": [
    "This paper uses the publicly available [MIMIC-III database](https://www.nature.com/articles/sdata201635) which contains clinical data in a critical care setting. After reviewing the paper in detail, we decided to use [MIMIC-Extract](https://arxiv.org/abs/1907.08322), an open source pipeline by (Wang et al., 2020) for transforming the raw EHR data into usable Pandas dataframes containing hourly time series of vitals and laboratory measurements after performing unit conversion, outlier handling, and aggregation of semantically similar features.\n",
    "\n",
    "Unfortunately, the MIMIC-Extract pipeline misses two features the [paper code](https://github.com/mit-caml/multitask-patients) makes use of:\n",
    "* `timecmo_chart` which indicates the timestamp of a patient when it has been declared in CMO (Comfort Measures Only) state. This feature comes from a MIMIC-III concept table called `code_status`.\n",
    "* `sapsii` which contains the SAPS (Simplified Acute Physiology Score) II. This feature comes from another MIMIC-III concept table called `sapsii`.\n",
    "\n",
    "As a result, there are three data files needed to run this notebook:\n",
    "* `all_hourly_data.h5`, an HDF file resulting from running the MIMIC-Extract pipeline which is publicly available in GCP using [this link](https://console.cloud.google.com/storage/browser/mimic_extract) and referenced in the [MIMIC-Extract github repo](https://github.com/MLforHealth/MIMIC_Extract).\n",
    "* `code_status.csv`, a CSV file holding the MIMIC concept table `CODE_STATUS` that can be generated following the instructions in [this link within the MIT-LCP github repo](https://github.com/MIT-LCP/mimic-code/tree/main/mimic-iii/concepts#generating-the-concepts-in-postgresql).\n",
    "* `sapsii.csv`, a CSV file holding the MIMIC concept table `SAPSII` that can be generated following the instructions in [this link within the MIT-LCP github repo](https://github.com/MIT-LCP/mimic-code/tree/main/mimic-iii/concepts#generating-the-concepts-in-postgresql).\n",
    "\n",
    "The functions used in this notebook assume the three files are in the folder `../data/` by default. However, location can be defined using arguments to the functions that process the data.\n",
    "\n",
    "All code needed to replicate the paper is in [our github repo](https://github.com/ger6-illini/dl4h-sp23-team27-project) inside a Python module called `mtl_patients`.\n",
    "\n",
    "The first function from that module we will start using is `get_summaries()`. This function provides three summaries in four dataframes which, in return order, are:\n",
    "* A summary providing some statistics of all patients broken by careunit.\n",
    "* A summary providing some statistics of all patients broken by SAPS-II score quartile.\n",
    "* A summary providing some statistics of the 29 distinct physiological measurements used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d024700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 10:15:09.692630: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "pathname = \"../code/\"\n",
    "if pathname not in sys.path:\n",
    "    sys.path.append(\"../code/\")\n",
    "\n",
    "from mtl_patients import get_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04a5c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.97 s, sys: 1.43 s, total: 8.4 s\n",
      "Wall time: 8.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pat_summ_by_cu_df, pat_summ_by_sapsiiq_df, vitals_labs_summ_df = get_summaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bcc93e",
   "metadata": {},
   "source": [
    "Let's now display the summaries one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b48560",
   "metadata": {},
   "source": [
    "### 1.1. Data summary by patients in each intensive care unit (ICU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "451bdc7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>n</th>\n",
       "      <th>Class Imbalance</th>\n",
       "      <th>Age (Mean)</th>\n",
       "      <th>Gender (Male)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Careunit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CCU</th>\n",
       "      <td>5193</td>\n",
       "      <td>790</td>\n",
       "      <td>0.152</td>\n",
       "      <td>83.31</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSRU</th>\n",
       "      <td>7050</td>\n",
       "      <td>223</td>\n",
       "      <td>0.032</td>\n",
       "      <td>69.54</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MICU</th>\n",
       "      <td>12207</td>\n",
       "      <td>2674</td>\n",
       "      <td>0.219</td>\n",
       "      <td>78.21</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SICU</th>\n",
       "      <td>5520</td>\n",
       "      <td>829</td>\n",
       "      <td>0.150</td>\n",
       "      <td>73.49</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSICU</th>\n",
       "      <td>4502</td>\n",
       "      <td>583</td>\n",
       "      <td>0.129</td>\n",
       "      <td>67.33</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall</th>\n",
       "      <td>34472</td>\n",
       "      <td>5099</td>\n",
       "      <td>0.148</td>\n",
       "      <td>75.03</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              N     n  Class Imbalance  Age (Mean)  Gender (Male)\n",
       "Careunit                                                         \n",
       "CCU        5193   790            0.152       83.31           0.58\n",
       "CSRU       7050   223            0.032       69.54           0.67\n",
       "MICU      12207  2674            0.219       78.21           0.51\n",
       "SICU       5520   829            0.150       73.49           0.51\n",
       "TSICU      4502   583            0.129       67.33           0.61\n",
       "Overall   34472  5099            0.148       75.03           0.57"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_summ_by_cu_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dff829",
   "metadata": {},
   "source": [
    "In the previous summary, patients were broken in groups where each group is one of the five careunits where patients were first admitted:\n",
    "* CCU: Coronary Care Unit\n",
    "* CSRU: Cardiac Surgery Recovery Unit\n",
    "* MICU: Medical Intensive Care Unit\n",
    "* SICU: Surgical Intensive Care Unit\n",
    "* TSICU: Trauma Surgical Intensive Care Unit\n",
    "\n",
    "In addition, an overall group was also added. The statistics provided by the summary are:\n",
    "* `N`: The number of samples (patients) in the group.\n",
    "* `n`: The number of samples (patients) where meeting the in-hospital mortality criteria defined in the paper: patient died or had a note of \"Do Not Resuscitate\" (DNR) or had a note of \"Comfort Measures Only\" (CMO).\n",
    "* `Class Imbalance`: Ratio of patients meeting the in-hospital mortality criteria defined in the paper, i.e., $\\dfrac{\\text{N}}{\\text{n}}$.\n",
    "* `Age (Mean)`: Mean age of patients for each group in years.\n",
    "* `Gender (Male)`: Ratio of patients that are males.\n",
    "\n",
    "This summary was prepared to match the Table 1 in the original paper. There are differences between both that can be attributed to the way how data was preprocessed by MIMIC-Extract when compared to the preprocessing done by the authors back in 2018, before MIMIC-Extract became available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a8e12",
   "metadata": {},
   "source": [
    "### 1.2. Data summary by patients in each SAPS-II score quartile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d1474f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>n</th>\n",
       "      <th>Class Imbalance</th>\n",
       "      <th>Age (Mean)</th>\n",
       "      <th>Gender (Male)</th>\n",
       "      <th>SAPS-II (Min)</th>\n",
       "      <th>SAPS-II (Mean)</th>\n",
       "      <th>SAPS-II (Max)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAPS-II Quartile</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7449</td>\n",
       "      <td>115</td>\n",
       "      <td>0.015</td>\n",
       "      <td>45.50</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "      <td>16.56</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10322</td>\n",
       "      <td>669</td>\n",
       "      <td>0.065</td>\n",
       "      <td>68.84</td>\n",
       "      <td>0.58</td>\n",
       "      <td>23</td>\n",
       "      <td>27.73</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8360</td>\n",
       "      <td>1274</td>\n",
       "      <td>0.152</td>\n",
       "      <td>86.70</td>\n",
       "      <td>0.55</td>\n",
       "      <td>33</td>\n",
       "      <td>36.72</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8341</td>\n",
       "      <td>3041</td>\n",
       "      <td>0.365</td>\n",
       "      <td>97.36</td>\n",
       "      <td>0.53</td>\n",
       "      <td>42</td>\n",
       "      <td>52.62</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall</th>\n",
       "      <td>34472</td>\n",
       "      <td>5099</td>\n",
       "      <td>0.148</td>\n",
       "      <td>75.03</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0</td>\n",
       "      <td>33.52</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      N     n  Class Imbalance  Age (Mean)  Gender (Male)  \\\n",
       "SAPS-II Quartile                                                            \n",
       "0                  7449   115            0.015       45.50           0.61   \n",
       "1                 10322   669            0.065       68.84           0.58   \n",
       "2                  8360  1274            0.152       86.70           0.55   \n",
       "3                  8341  3041            0.365       97.36           0.53   \n",
       "Overall           34472  5099            0.148       75.03           0.57   \n",
       "\n",
       "                  SAPS-II (Min)  SAPS-II (Mean)  SAPS-II (Max)  \n",
       "SAPS-II Quartile                                                \n",
       "0                             0           16.56             22  \n",
       "1                            23           27.73             32  \n",
       "2                            33           36.72             41  \n",
       "3                            42           52.62            118  \n",
       "Overall                       0           33.52            118  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_summ_by_sapsiiq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711c274",
   "metadata": {},
   "source": [
    "In the previous summary, patients were broken based on the quartile of the SAPS-II score assigned to them. As it can be seen, the two quartiles have the ranges $[0, 22], [23, 32], [33, 41], [42, 118] $. This was included in the authors code but not in the paper. It seems the class imbalance might have been the primary reason. As it is evident from the summary, most of the patients are in quartile $3$ since they are in an ICU and is expected their values are on the high side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b0959c",
   "metadata": {},
   "source": [
    "### 1.3. Data summary for physiological measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2e18ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>avg</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>N</th>\n",
       "      <th>pres.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vital/Lab Measurement</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anion gap</th>\n",
       "      <td>5.00</td>\n",
       "      <td>13.72</td>\n",
       "      <td>50.00</td>\n",
       "      <td>3.99</td>\n",
       "      <td>183732</td>\n",
       "      <td>0.0835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bicarbonate</th>\n",
       "      <td>0.00</td>\n",
       "      <td>24.23</td>\n",
       "      <td>53.00</td>\n",
       "      <td>4.74</td>\n",
       "      <td>192632</td>\n",
       "      <td>0.0875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blood urea nitrogen</th>\n",
       "      <td>0.00</td>\n",
       "      <td>26.21</td>\n",
       "      <td>250.00</td>\n",
       "      <td>21.75</td>\n",
       "      <td>194596</td>\n",
       "      <td>0.0884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chloride</th>\n",
       "      <td>50.00</td>\n",
       "      <td>105.22</td>\n",
       "      <td>175.00</td>\n",
       "      <td>6.31</td>\n",
       "      <td>211525</td>\n",
       "      <td>0.0961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creatinine</th>\n",
       "      <td>0.10</td>\n",
       "      <td>1.39</td>\n",
       "      <td>46.60</td>\n",
       "      <td>1.48</td>\n",
       "      <td>195429</td>\n",
       "      <td>0.0888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diastolic blood pressure</th>\n",
       "      <td>0.00</td>\n",
       "      <td>60.89</td>\n",
       "      <td>307.00</td>\n",
       "      <td>14.13</td>\n",
       "      <td>1908674</td>\n",
       "      <td>0.8672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fraction inspired oxygen</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>98315</td>\n",
       "      <td>0.0447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glascow coma scale total</th>\n",
       "      <td>3.00</td>\n",
       "      <td>12.49</td>\n",
       "      <td>15.00</td>\n",
       "      <td>3.59</td>\n",
       "      <td>377787</td>\n",
       "      <td>0.1716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glucose</th>\n",
       "      <td>33.00</td>\n",
       "      <td>140.49</td>\n",
       "      <td>1591.00</td>\n",
       "      <td>57.22</td>\n",
       "      <td>512585</td>\n",
       "      <td>0.2329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart rate</th>\n",
       "      <td>0.00</td>\n",
       "      <td>84.97</td>\n",
       "      <td>300.00</td>\n",
       "      <td>17.27</td>\n",
       "      <td>1971748</td>\n",
       "      <td>0.8959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hematocrit</th>\n",
       "      <td>0.00</td>\n",
       "      <td>31.02</td>\n",
       "      <td>71.70</td>\n",
       "      <td>5.35</td>\n",
       "      <td>253794</td>\n",
       "      <td>0.1153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hemoglobin</th>\n",
       "      <td>0.00</td>\n",
       "      <td>10.64</td>\n",
       "      <td>22.10</td>\n",
       "      <td>1.90</td>\n",
       "      <td>205340</td>\n",
       "      <td>0.0933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lactate</th>\n",
       "      <td>0.40</td>\n",
       "      <td>2.67</td>\n",
       "      <td>30.00</td>\n",
       "      <td>2.63</td>\n",
       "      <td>61207</td>\n",
       "      <td>0.0278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>magnesium</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.05</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.41</td>\n",
       "      <td>181135</td>\n",
       "      <td>0.0823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean blood pressure</th>\n",
       "      <td>14.00</td>\n",
       "      <td>79.37</td>\n",
       "      <td>330.00</td>\n",
       "      <td>15.49</td>\n",
       "      <td>1899156</td>\n",
       "      <td>0.8629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oxygen saturation</th>\n",
       "      <td>0.00</td>\n",
       "      <td>96.73</td>\n",
       "      <td>100.00</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1884351</td>\n",
       "      <td>0.8562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial thromboplastin time</th>\n",
       "      <td>18.80</td>\n",
       "      <td>41.19</td>\n",
       "      <td>150.00</td>\n",
       "      <td>24.64</td>\n",
       "      <td>135753</td>\n",
       "      <td>0.0617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ph</th>\n",
       "      <td>6.50</td>\n",
       "      <td>7.38</td>\n",
       "      <td>8.40</td>\n",
       "      <td>0.08</td>\n",
       "      <td>201128</td>\n",
       "      <td>0.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phosphate</th>\n",
       "      <td>0.50</td>\n",
       "      <td>3.49</td>\n",
       "      <td>20.00</td>\n",
       "      <td>1.42</td>\n",
       "      <td>118669</td>\n",
       "      <td>0.0539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>platelets</th>\n",
       "      <td>0.00</td>\n",
       "      <td>204.99</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>113.45</td>\n",
       "      <td>186591</td>\n",
       "      <td>0.0848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potassium</th>\n",
       "      <td>0.80</td>\n",
       "      <td>4.13</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>235712</td>\n",
       "      <td>0.1071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prothrombin time inr</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1.52</td>\n",
       "      <td>88.80</td>\n",
       "      <td>1.24</td>\n",
       "      <td>129199</td>\n",
       "      <td>0.0587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prothrombin time pt</th>\n",
       "      <td>2.39</td>\n",
       "      <td>15.98</td>\n",
       "      <td>150.00</td>\n",
       "      <td>6.97</td>\n",
       "      <td>129174</td>\n",
       "      <td>0.0587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respiratory rate</th>\n",
       "      <td>0.00</td>\n",
       "      <td>19.09</td>\n",
       "      <td>300.00</td>\n",
       "      <td>5.72</td>\n",
       "      <td>1940454</td>\n",
       "      <td>0.8816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sodium</th>\n",
       "      <td>50.00</td>\n",
       "      <td>138.62</td>\n",
       "      <td>225.00</td>\n",
       "      <td>5.27</td>\n",
       "      <td>223341</td>\n",
       "      <td>0.1015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>systolic blood pressure</th>\n",
       "      <td>0.00</td>\n",
       "      <td>121.83</td>\n",
       "      <td>311.00</td>\n",
       "      <td>21.99</td>\n",
       "      <td>1909144</td>\n",
       "      <td>0.8674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature</th>\n",
       "      <td>26.00</td>\n",
       "      <td>36.98</td>\n",
       "      <td>42.22</td>\n",
       "      <td>0.78</td>\n",
       "      <td>648358</td>\n",
       "      <td>0.2946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>0.00</td>\n",
       "      <td>83.07</td>\n",
       "      <td>250.00</td>\n",
       "      <td>23.36</td>\n",
       "      <td>61858</td>\n",
       "      <td>0.0281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white blood cell count</th>\n",
       "      <td>0.00</td>\n",
       "      <td>11.94</td>\n",
       "      <td>939.00</td>\n",
       "      <td>10.01</td>\n",
       "      <td>178844</td>\n",
       "      <td>0.0813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               min     avg      max     std        N   pres.\n",
       "Vital/Lab Measurement                                                       \n",
       "anion gap                     5.00   13.72    50.00    3.99   183732  0.0835\n",
       "bicarbonate                   0.00   24.23    53.00    4.74   192632  0.0875\n",
       "blood urea nitrogen           0.00   26.21   250.00   21.75   194596  0.0884\n",
       "chloride                     50.00  105.22   175.00    6.31   211525  0.0961\n",
       "creatinine                    0.10    1.39    46.60    1.48   195429  0.0888\n",
       "diastolic blood pressure      0.00   60.89   307.00   14.13  1908674  0.8672\n",
       "fraction inspired oxygen      0.21    0.53     1.00    0.19    98315  0.0447\n",
       "glascow coma scale total      3.00   12.49    15.00    3.59   377787  0.1716\n",
       "glucose                      33.00  140.49  1591.00   57.22   512585  0.2329\n",
       "heart rate                    0.00   84.97   300.00   17.27  1971748  0.8959\n",
       "hematocrit                    0.00   31.02    71.70    5.35   253794  0.1153\n",
       "hemoglobin                    0.00   10.64    22.10    1.90   205340  0.0933\n",
       "lactate                       0.40    2.67    30.00    2.63    61207  0.0278\n",
       "magnesium                     0.00    2.05    20.00    0.41   181135  0.0823\n",
       "mean blood pressure          14.00   79.37   330.00   15.49  1899156  0.8629\n",
       "oxygen saturation             0.00   96.73   100.00    3.59  1884351  0.8562\n",
       "partial thromboplastin time  18.80   41.19   150.00   24.64   135753  0.0617\n",
       "ph                            6.50    7.38     8.40    0.08   201128  0.0914\n",
       "phosphate                     0.50    3.49    20.00    1.42   118669  0.0539\n",
       "platelets                     0.00  204.99  2000.00  113.45   186591  0.0848\n",
       "potassium                     0.80    4.13    12.00    0.65   235712  0.1071\n",
       "prothrombin time inr          0.50    1.52    88.80    1.24   129199  0.0587\n",
       "prothrombin time pt           2.39   15.98   150.00    6.97   129174  0.0587\n",
       "respiratory rate              0.00   19.09   300.00    5.72  1940454  0.8816\n",
       "sodium                       50.00  138.62   225.00    5.27   223341  0.1015\n",
       "systolic blood pressure       0.00  121.83   311.00   21.99  1909144  0.8674\n",
       "temperature                  26.00   36.98    42.22    0.78   648358  0.2946\n",
       "weight                        0.00   83.07   250.00   23.36    61858  0.0281\n",
       "white blood cell count        0.00   11.94   939.00   10.01   178844  0.0813"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vitals_labs_summ_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166cab69",
   "metadata": {},
   "source": [
    "In the previous summary, all vitals and lab measurements selected in the paper (29 in total) are listed with relevant statistics associated to it:\n",
    "* `min` representing the minimum of the measurement observed in the vitals/labs.\n",
    "* `avg` representing the average of the measurement observed in the vitals/labs.\n",
    "* `max` representing the maximum of the measurement observed in the vitals/labs.\n",
    "* `std` representing the standard deviation of the measurement observed in the vitals/labs.\n",
    "* `N` representing the number of non `NaN` samples for the specific vital/lab measurement.\n",
    "* `pres.` representing the portion of all possible hours across all patients, admissions, and ICU stays where at least one of the 104 vitals/labs measurements in the original MIMIC-Extract pipeline was taken.\n",
    "\n",
    "All these measurements are based on the `vitals_labs_mean` dataframe in the MIMIC-Extract pipeline which provides average of vitals/labs on a per hour basis for each patient after going into an ICU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b21a0",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"section-2\">2. Methods & Results</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0a385d",
   "metadata": {},
   "source": [
    "The paper uses a two-step pipeline to: 1) identify relevant patient cohorts, and 2) use those relevant cohorts as separate tasks in a multi-lask learning framework to predict in-hospital mortality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55e340",
   "metadata": {},
   "source": [
    "### 2.1. Identifying Relevant Patient Cohorts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ac0335",
   "metadata": {},
   "source": [
    "In this first step, the raw patient data needs to be prepared in such a way that the result is a 3D matrix of shape $(P \\times T \\times F)$ where $P$ represents the number of patients, $T$ the number of timesteps, and $F$ the number of features as depicted in the figure below (in blue) which is partially based on Figure 2 of the original paper. All numbers shown in the figure below correspond to a specific experiment published in the paper in which the observation window is limited to the first $24$ hours (cutoff periord) after a patient goes into a careunit and there is a gap of $12$ hours (gap period) between the end of the observation window and the beginning of the prediction window where the prediction task is in-hospital mortality.\n",
    "\n",
    "Preparation of the data to get the 3D (blue) matrix is performed by a function called `prepare_data()` inside the `mtl_patients.py` module. This preparation consists of the following transformations taken from the paper and the reference implementation:\n",
    "\n",
    "1. Calculation of the mortality flag (prediction label) and mortality time for every patient in the dataset using an *extended* definition of mortality: death, a note of \"Do Not Resuscitate\" (DNR), or a note of \"Comfort Measures Only\" (CMO). In case any of these conditions are met for a patient, the corresponding mortality label is set to *True* and the corresponding mortality time is considered as the earliest time of any of the three conditions.\n",
    "2. Data used for the prediction is only limited to the first certain amount of hours after a patient goes into the ICU. This amount of hours is called inside the code \"a cutoff period\" (observation window) and defines the period of data used to train all models. In addition, there is another number of hours called inside the code \"the gap period\" which represents the time between the end of the observation window and the beginning of the prediction window to prevent label leakage. All patients that died under the *extended* definition before the cutoff period plus the gap period or stayed less than the cutoff period are excluded from the experiment as part of this step. Also, all patients under the age of 15 are excluded (this is already part of the exclusion criteria of the MIMIC-Extract pipeline).\n",
    "3. There are 29 vitals/labs timeseries selected by the paper. Only data within the cutoff period for vitals/labs is kept and rest is removed. This will be used for the rest of the machine learning pipeline.\n",
    "4. All vitals/labs values are converted to z-scores so they all have zero mean and unit standard deviation. Those z-scores are rounded to the closest integer and clipped between $-4$ and $4$ or set to $9$ in case of `NaN`. This allows to map every vital/lab measurement (a float) to one of ten possible values $[-4, -3, -2, -1, 0, 1, 2, 3, 4, 9]$, so they can be converted to dummy values. After dummifying the vitals/labs, column for the $9$ (`NaN`) is removed, and the resulting matrix is sparse and containing either $0$s or $1$s.\n",
    "5. Every patient is padded with rows of zeroes for those hours that are missed. For example, if a patient only has vitals/labs for the first ten hours and the cutoff period is 24, code adds fourteen hours (rows) with zeroes for that patient. In the end, the matrix will have a size of $P \\times T \\times F$ as expected by the subsequent models.\n",
    "6. Finally, static data (gender, age, and ethnicity) is converted to integers representing categories and dummified. In case of age, there are four buckets established; $(10, 30), (30, 50), (50, 70), (70, \\infty)$; while ethnicity is broken into five buckets (asian, white, hispanic, black, other).\n",
    "7. Cohort assignments based on first careunit or Simplified Acute Physiology Score (SAPS) II score quartile is calculated for each patient and returned as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d014e8",
   "metadata": {},
   "source": [
    "![Figure 1](../img/paper-181-fig-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040a251",
   "metadata": {},
   "source": [
    "The `discover_cohorts()` function inside the `mtl_patients.py` module is the one implementing the pipeline shown in the figure above and the calling the `prepare_data()` function detailed previously as the first step. Once data has been processed, the function will break the data in training, validation, and test data sets in a $70\\%/10\\%/20\\%$ proportion.\n",
    "\n",
    "The training data is used to train an LSTM autoencoder. The main purpose of the LSTM autoencoder is to generate a fixed-length dense representation (embedding) of the sparse inputs trying. The paper selected embeddings of size $50$ as the optimal dimension (hyperparameter). The purple box in the middle of the diagram above (a 2D matrix) represents the embeddings after the LSTM autoencoder learned the representation of the original 3D matrix of shape $(32537 \\times 24 \\times 232)$ where every row corresponds to a patient.\n",
    "\n",
    "Once the embeddings are calculated, a Gaussian Mixture Model is applied using $3$ clusters (the value the authors considered optimal). The result are the three green boxes representing three cohorts discovered in an unsupervised way and grouping similar patients based on the three static and the 29 time-varying vitals/labs selected from the MIMIC-III database.\n",
    "\n",
    "Below, the line of code calling the `discover_cohorts()` function and returning the NumPy array with cohort membership, consisting of three groups, for each patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81181566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtl_patients import discover_cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f8b82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 10:16:10.066969: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM autoencoder started...\n",
      "Epoch 1/100\n",
      "178/178 [==============================] - 14s 70ms/step - loss: 0.0405 - val_loss: 0.0360\n",
      "Epoch 2/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0347 - val_loss: 0.0333\n",
      "Epoch 3/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0324 - val_loss: 0.0318\n",
      "Epoch 4/100\n",
      "178/178 [==============================] - 12s 66ms/step - loss: 0.0312 - val_loss: 0.0308\n",
      "Epoch 5/100\n",
      "178/178 [==============================] - 12s 66ms/step - loss: 0.0303 - val_loss: 0.0299\n",
      "Epoch 6/100\n",
      "178/178 [==============================] - 12s 67ms/step - loss: 0.0295 - val_loss: 0.0293\n",
      "Epoch 7/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0291 - val_loss: 0.0289\n",
      "Epoch 8/100\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.0287 - val_loss: 0.0286\n",
      "Epoch 9/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0284 - val_loss: 0.0284\n",
      "Epoch 10/100\n",
      "178/178 [==============================] - 12s 66ms/step - loss: 0.0282 - val_loss: 0.0282\n",
      "Epoch 11/100\n",
      "178/178 [==============================] - 12s 66ms/step - loss: 0.0280 - val_loss: 0.0280\n",
      "Epoch 12/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0278 - val_loss: 0.0278\n",
      "Epoch 13/100\n",
      "178/178 [==============================] - 12s 67ms/step - loss: 0.0276 - val_loss: 0.0276\n",
      "Epoch 14/100\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.0274 - val_loss: 0.0274\n",
      "Epoch 15/100\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.0272 - val_loss: 0.0272\n",
      "Epoch 16/100\n",
      "178/178 [==============================] - 11s 62ms/step - loss: 0.0271 - val_loss: 0.0271\n",
      "Epoch 17/100\n",
      "178/178 [==============================] - 11s 61ms/step - loss: 0.0269 - val_loss: 0.0269\n",
      "Epoch 18/100\n",
      "178/178 [==============================] - 11s 61ms/step - loss: 0.0267 - val_loss: 0.0267\n",
      "Epoch 19/100\n",
      "178/178 [==============================] - 11s 62ms/step - loss: 0.0266 - val_loss: 0.0266\n",
      "Epoch 20/100\n",
      "178/178 [==============================] - 11s 61ms/step - loss: 0.0264 - val_loss: 0.0264\n",
      "Epoch 21/100\n",
      "178/178 [==============================] - 11s 63ms/step - loss: 0.0263 - val_loss: 0.0263\n",
      "Epoch 22/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0261 - val_loss: 0.0262\n",
      "Epoch 23/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0260 - val_loss: 0.0260\n",
      "Epoch 24/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0259 - val_loss: 0.0259\n",
      "Epoch 25/100\n",
      "178/178 [==============================] - 12s 67ms/step - loss: 0.0258 - val_loss: 0.0258\n",
      "Epoch 26/100\n",
      "178/178 [==============================] - 11s 63ms/step - loss: 0.0256 - val_loss: 0.0257\n",
      "Epoch 27/100\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.0255 - val_loss: 0.0256\n",
      "Epoch 28/100\n",
      "178/178 [==============================] - 11s 63ms/step - loss: 0.0254 - val_loss: 0.0255\n",
      "Epoch 29/100\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.0254 - val_loss: 0.0254\n",
      "Epoch 30/100\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.0253 - val_loss: 0.0254\n",
      "Epoch 31/100\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.0252 - val_loss: 0.0253\n",
      "Epoch 32/100\n",
      "178/178 [==============================] - 11s 65ms/step - loss: 0.0251 - val_loss: 0.0252\n",
      "Epoch 33/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0251 - val_loss: 0.0251\n",
      "Epoch 34/100\n",
      "178/178 [==============================] - 12s 66ms/step - loss: 0.0250 - val_loss: 0.0251\n",
      "Epoch 35/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0249 - val_loss: 0.0250\n",
      "Epoch 36/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0249 - val_loss: 0.0250\n",
      "Epoch 37/100\n",
      "178/178 [==============================] - 12s 66ms/step - loss: 0.0248 - val_loss: 0.0249\n",
      "Epoch 38/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0248 - val_loss: 0.0249\n",
      "Epoch 39/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0247 - val_loss: 0.0248\n",
      "Epoch 40/100\n",
      "178/178 [==============================] - 12s 66ms/step - loss: 0.0247 - val_loss: 0.0248\n",
      "Epoch 41/100\n",
      "178/178 [==============================] - 12s 66ms/step - loss: 0.0247 - val_loss: 0.0247\n",
      "Epoch 42/100\n",
      "178/178 [==============================] - 12s 66ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 43/100\n",
      "178/178 [==============================] - 12s 66ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 44/100\n",
      "178/178 [==============================] - 12s 66ms/step - loss: 0.0245 - val_loss: 0.0246\n",
      "Epoch 45/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0245 - val_loss: 0.0246\n",
      "Epoch 46/100\n",
      "178/178 [==============================] - 12s 66ms/step - loss: 0.0245 - val_loss: 0.0246\n",
      "Epoch 47/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0244 - val_loss: 0.0245\n",
      "Epoch 48/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0244 - val_loss: 0.0245\n",
      "Epoch 49/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0244 - val_loss: 0.0245\n",
      "Epoch 50/100\n",
      "178/178 [==============================] - 12s 66ms/step - loss: 0.0243 - val_loss: 0.0244\n",
      "Epoch 51/100\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.0243 - val_loss: 0.0244\n",
      "Epoch 52/100\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.0243 - val_loss: 0.0244\n",
      "Epoch 53/100\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.0242 - val_loss: 0.0243\n",
      "Epoch 54/100\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.0242 - val_loss: 0.0243\n",
      "Epoch 55/100\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.0242 - val_loss: 0.0243\n",
      "Epoch 56/100\n",
      "178/178 [==============================] - 11s 63ms/step - loss: 0.0241 - val_loss: 0.0242\n",
      "Epoch 57/100\n",
      "178/178 [==============================] - 11s 62ms/step - loss: 0.0241 - val_loss: 0.0242\n",
      "Epoch 58/100\n",
      "178/178 [==============================] - 11s 63ms/step - loss: 0.0241 - val_loss: 0.0242\n",
      "Epoch 59/100\n",
      "178/178 [==============================] - 11s 63ms/step - loss: 0.0241 - val_loss: 0.0242\n",
      "Epoch 60/100\n",
      "178/178 [==============================] - 11s 62ms/step - loss: 0.0240 - val_loss: 0.0241\n",
      "Epoch 61/100\n",
      "178/178 [==============================] - 11s 63ms/step - loss: 0.0240 - val_loss: 0.0241\n",
      "Epoch 62/100\n",
      "178/178 [==============================] - 11s 62ms/step - loss: 0.0240 - val_loss: 0.0241\n",
      "Epoch 63/100\n",
      "178/178 [==============================] - 11s 62ms/step - loss: 0.0240 - val_loss: 0.0241\n",
      "Epoch 64/100\n",
      "178/178 [==============================] - 11s 62ms/step - loss: 0.0239 - val_loss: 0.0240\n",
      "Epoch 65/100\n",
      "178/178 [==============================] - 11s 62ms/step - loss: 0.0239 - val_loss: 0.0240\n",
      "Epoch 66/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0239 - val_loss: 0.0240\n",
      "Epoch 67/100\n",
      "178/178 [==============================] - 11s 65ms/step - loss: 0.0239 - val_loss: 0.0240\n",
      "Epoch 68/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0238 - val_loss: 0.0240\n",
      "Epoch 69/100\n",
      "178/178 [==============================] - 11s 63ms/step - loss: 0.0238 - val_loss: 0.0239\n",
      "Epoch 70/100\n",
      "178/178 [==============================] - 11s 65ms/step - loss: 0.0238 - val_loss: 0.0239\n",
      "Epoch 71/100\n",
      "178/178 [==============================] - 11s 63ms/step - loss: 0.0238 - val_loss: 0.0239\n",
      "Epoch 72/100\n",
      "178/178 [==============================] - 12s 70ms/step - loss: 0.0238 - val_loss: 0.0239\n",
      "Epoch 73/100\n",
      "178/178 [==============================] - 13s 70ms/step - loss: 0.0237 - val_loss: 0.0238\n",
      "Epoch 74/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0237 - val_loss: 0.0238\n",
      "Epoch 75/100\n",
      "178/178 [==============================] - 12s 66ms/step - loss: 0.0237 - val_loss: 0.0238\n",
      "Epoch 76/100\n",
      "178/178 [==============================] - 12s 70ms/step - loss: 0.0237 - val_loss: 0.0238\n",
      "Epoch 77/100\n",
      "178/178 [==============================] - 12s 69ms/step - loss: 0.0237 - val_loss: 0.0238\n",
      "Epoch 78/100\n",
      "178/178 [==============================] - 12s 67ms/step - loss: 0.0236 - val_loss: 0.0237\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 13s 71ms/step - loss: 0.0236 - val_loss: 0.0238\n",
      "Epoch 80/100\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.0236 - val_loss: 0.0237\n",
      "Epoch 81/100\n",
      "178/178 [==============================] - 13s 75ms/step - loss: 0.0236 - val_loss: 0.0237\n",
      "Epoch 82/100\n",
      "178/178 [==============================] - 13s 74ms/step - loss: 0.0236 - val_loss: 0.0237\n",
      "Epoch 83/100\n",
      "178/178 [==============================] - 13s 74ms/step - loss: 0.0235 - val_loss: 0.0237\n",
      "Epoch 84/100\n",
      "178/178 [==============================] - 12s 68ms/step - loss: 0.0235 - val_loss: 0.0236\n",
      "Epoch 85/100\n",
      "178/178 [==============================] - 11s 63ms/step - loss: 0.0235 - val_loss: 0.0236\n",
      "Epoch 86/100\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.0235 - val_loss: 0.0236\n",
      "Epoch 87/100\n",
      "178/178 [==============================] - 12s 68ms/step - loss: 0.0235 - val_loss: 0.0236\n",
      "Epoch 88/100\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.0235 - val_loss: 0.0236\n",
      "Epoch 89/100\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.0234 - val_loss: 0.0236\n",
      "Epoch 90/100\n",
      "178/178 [==============================] - 11s 62ms/step - loss: 0.0234 - val_loss: 0.0235\n",
      "Epoch 91/100\n",
      "178/178 [==============================] - 13s 71ms/step - loss: 0.0234 - val_loss: 0.0235\n",
      "Epoch 92/100\n",
      "178/178 [==============================] - 12s 69ms/step - loss: 0.0234 - val_loss: 0.0235\n",
      "Epoch 93/100\n",
      "178/178 [==============================] - 11s 65ms/step - loss: 0.0234 - val_loss: 0.0235\n",
      "Epoch 94/100\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.0234 - val_loss: 0.0235\n",
      "Epoch 95/100\n",
      "178/178 [==============================] - 12s 68ms/step - loss: 0.0233 - val_loss: 0.0235\n",
      "Epoch 96/100\n",
      "178/178 [==============================] - 12s 67ms/step - loss: 0.0233 - val_loss: 0.0235\n",
      "Epoch 97/100\n",
      "178/178 [==============================] - 11s 62ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 98/100\n",
      "178/178 [==============================] - 11s 65ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 99/100\n",
      "178/178 [==============================] - 11s 62ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 100/100\n",
      "178/178 [==============================] - 11s 62ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Training LSTM autoencoder trained!\n",
      "712/712 [==============================] - 2s 3ms/step\n",
      "1017/1017 [==============================] - 3s 3ms/step\n",
      "Patient embeddings created! Shape: (32537, 50)\n",
      "Training Gaussian Mixture Model...\n",
      "Initialization 0\n",
      "  Iteration 10\n",
      "  Iteration 20\n",
      "Initialization converged: True\n",
      "Gaussian Mixture Model applied to embeddings! Results shape: (32537,)\n",
      "Cluster results saved to '../data/unsupervised_clusters.npy'\n",
      "CPU times: user 1h 22min 5s, sys: 6min 1s, total: 1h 28min 7s\n",
      "Wall time: 19min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cohort_unsupervised = discover_cohorts(cutoff_hours=24, gap_hours=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a684375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtl_patients import run_mortality_prediction_task, prepare_data, create_single_task_learning_model\n",
    "from mtl_patients import stratified_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdada353",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, cohort_careunits, cohort_sapsii_quartile, subject_ids = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95341bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type='global'\n",
    "cutoff_hours=24\n",
    "gap_hours=12\n",
    "cohort_criteria_to_select='careunits'\n",
    "train_val_random_seed=0\n",
    "cohort_unsupervised_filename='../data/unsupervised_clusters.npy'\n",
    "lstm_layer_size=16\n",
    "epochs=100\n",
    "learning_rate=0.0001\n",
    "use_cohort_inv_freq_weights=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6208d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cohort_criteria_to_select == 'careunits':\n",
    "    cohort_criteria = cohort_careunits\n",
    "elif cohort_criteria_to_select == 'sapsii_quartile':\n",
    "    cohort_criteria = cohort_sapsii_quartile\n",
    "elif cohort_criteria_to_select == 'unsupervised':\n",
    "    cohort_criteria = np.load(f\"{cohort_unsupervised_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe266183",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, cohorts_train, cohorts_val, cohorts_test = \\\n",
    "    stratified_split(X, Y, cohort_criteria, train_val_random_seed=train_val_random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bf27990",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = np.unique(cohorts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63c00b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CCU', 'CSRU', 'MICU', 'SICU', 'TSICU'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "364a3c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of patients in cohort CCU is 3362\n",
      "# of patients in cohort CSRU is 4858\n",
      "# of patients in cohort MICU is 7976\n",
      "# of patients in cohort SICU is 3633\n",
      "# of patients in cohort TSICU is 2946\n"
     ]
    }
   ],
   "source": [
    "task_weights = {}    \n",
    "for cohort in tasks:\n",
    "    num_samples_in_cohort = len(np.where(cohorts_train == cohort)[0])\n",
    "    print(f\"# of patients in cohort {cohort} is {str(num_samples_in_cohort)}\")\n",
    "    task_weights[cohort] = len(X_train) / num_samples_in_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03d16db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e7d6720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 16)                15936     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,953\n",
      "Trainable params: 15,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = create_single_task_learning_model(lstm_layer_size=lstm_layer_size, input_dims=X_train.shape[1:],\n",
    "                                          output_dims=1, learning_rate=learning_rate)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71723d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6900d2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22775, 24, 232)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d36d9abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22775,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d436d2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.4781 - accuracy: 0.8537\n",
      "Epoch 2/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.4028 - accuracy: 0.8692\n",
      "Epoch 3/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3979 - accuracy: 0.8692\n",
      "Epoch 4/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3951 - accuracy: 0.8692\n",
      "Epoch 5/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.3935 - accuracy: 0.8692\n",
      "Epoch 6/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3919 - accuracy: 0.8692\n",
      "Epoch 7/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3908 - accuracy: 0.8692\n",
      "Epoch 8/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3898 - accuracy: 0.8692\n",
      "Epoch 9/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3890 - accuracy: 0.8692\n",
      "Epoch 10/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3882 - accuracy: 0.8692\n",
      "Epoch 11/100\n",
      "228/228 [==============================] - 1s 7ms/step - loss: 0.3876 - accuracy: 0.8692\n",
      "Epoch 12/100\n",
      "228/228 [==============================] - 1s 7ms/step - loss: 0.3869 - accuracy: 0.8692\n",
      "Epoch 13/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3862 - accuracy: 0.8692\n",
      "Epoch 14/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3858 - accuracy: 0.8692\n",
      "Epoch 15/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3850 - accuracy: 0.8692\n",
      "Epoch 16/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3844 - accuracy: 0.8692\n",
      "Epoch 17/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3838 - accuracy: 0.8692\n",
      "Epoch 18/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3833 - accuracy: 0.8692\n",
      "Epoch 19/100\n",
      "228/228 [==============================] - 1s 7ms/step - loss: 0.3828 - accuracy: 0.8692\n",
      "Epoch 20/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3823 - accuracy: 0.8692\n",
      "Epoch 21/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3818 - accuracy: 0.8692\n",
      "Epoch 22/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3815 - accuracy: 0.8692\n",
      "Epoch 23/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3809 - accuracy: 0.8692\n",
      "Epoch 24/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3803 - accuracy: 0.8692\n",
      "Epoch 25/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3799 - accuracy: 0.8692\n",
      "Epoch 26/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3795 - accuracy: 0.8692\n",
      "Epoch 27/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3790 - accuracy: 0.8692\n",
      "Epoch 28/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3786 - accuracy: 0.8692\n",
      "Epoch 29/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3781 - accuracy: 0.8692\n",
      "Epoch 30/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.3777 - accuracy: 0.8692\n",
      "Epoch 31/100\n",
      "228/228 [==============================] - 1s 7ms/step - loss: 0.3773 - accuracy: 0.8692\n",
      "Epoch 32/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.3768 - accuracy: 0.8692\n",
      "Epoch 33/100\n",
      "228/228 [==============================] - 1s 7ms/step - loss: 0.3764 - accuracy: 0.8692\n",
      "Epoch 34/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3759 - accuracy: 0.8692\n",
      "Epoch 35/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.3756 - accuracy: 0.8692\n",
      "Epoch 36/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.3750 - accuracy: 0.8692\n",
      "Epoch 37/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3745 - accuracy: 0.8692\n",
      "Epoch 38/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.3741 - accuracy: 0.8692\n",
      "Epoch 39/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.3737 - accuracy: 0.8692\n",
      "Epoch 40/100\n",
      "228/228 [==============================] - 1s 7ms/step - loss: 0.3735 - accuracy: 0.8692\n",
      "Epoch 41/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3726 - accuracy: 0.8692\n",
      "Epoch 42/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3724 - accuracy: 0.8692\n",
      "Epoch 43/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3720 - accuracy: 0.8692\n",
      "Epoch 44/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3715 - accuracy: 0.8692\n",
      "Epoch 45/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.3709 - accuracy: 0.8692\n",
      "Epoch 46/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.3704 - accuracy: 0.8692\n",
      "Epoch 47/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3701 - accuracy: 0.8692\n",
      "Epoch 48/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3694 - accuracy: 0.8691\n",
      "Epoch 49/100\n",
      "228/228 [==============================] - 1s 7ms/step - loss: 0.3689 - accuracy: 0.8691\n",
      "Epoch 50/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.3684 - accuracy: 0.8692\n",
      "Epoch 51/100\n",
      "228/228 [==============================] - 1s 7ms/step - loss: 0.3679 - accuracy: 0.8693\n",
      "Epoch 52/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.3674 - accuracy: 0.8694\n",
      "Epoch 53/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3672 - accuracy: 0.8694\n",
      "Epoch 54/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3664 - accuracy: 0.8696\n",
      "Epoch 55/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3659 - accuracy: 0.8694\n",
      "Epoch 56/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3653 - accuracy: 0.8695\n",
      "Epoch 57/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3646 - accuracy: 0.8697\n",
      "Epoch 58/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3643 - accuracy: 0.8697\n",
      "Epoch 59/100\n",
      "228/228 [==============================] - 1s 7ms/step - loss: 0.3638 - accuracy: 0.8696\n",
      "Epoch 60/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3636 - accuracy: 0.8699\n",
      "Epoch 61/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3628 - accuracy: 0.8700\n",
      "Epoch 62/100\n",
      "228/228 [==============================] - 1s 7ms/step - loss: 0.3621 - accuracy: 0.8700\n",
      "Epoch 63/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3617 - accuracy: 0.8701\n",
      "Epoch 64/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3611 - accuracy: 0.8703\n",
      "Epoch 65/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3609 - accuracy: 0.8700\n",
      "Epoch 66/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3601 - accuracy: 0.8703\n",
      "Epoch 67/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.3593 - accuracy: 0.8703\n",
      "Epoch 68/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3590 - accuracy: 0.8703\n",
      "Epoch 69/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3587 - accuracy: 0.8704\n",
      "Epoch 70/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3579 - accuracy: 0.8706\n",
      "Epoch 71/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3572 - accuracy: 0.8705\n",
      "Epoch 72/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3568 - accuracy: 0.8706\n",
      "Epoch 73/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3561 - accuracy: 0.8709\n",
      "Epoch 74/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3557 - accuracy: 0.8709\n",
      "Epoch 75/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3550 - accuracy: 0.8709\n",
      "Epoch 76/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.3548 - accuracy: 0.8714\n",
      "Epoch 77/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3541 - accuracy: 0.8714\n",
      "Epoch 78/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3534 - accuracy: 0.8712\n",
      "Epoch 79/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3529 - accuracy: 0.8715\n",
      "Epoch 80/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3523 - accuracy: 0.8716\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228/228 [==============================] - 1s 7ms/step - loss: 0.3518 - accuracy: 0.8715\n",
      "Epoch 82/100\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.3515 - accuracy: 0.8716\n",
      "Epoch 83/100\n",
      "228/228 [==============================] - 1s 7ms/step - loss: 0.3505 - accuracy: 0.8717\n",
      "Epoch 84/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3499 - accuracy: 0.8721\n",
      "Epoch 85/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3496 - accuracy: 0.8719\n",
      "Epoch 86/100\n",
      "228/228 [==============================] - 1s 7ms/step - loss: 0.3489 - accuracy: 0.8721\n",
      "Epoch 87/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3481 - accuracy: 0.8719\n",
      "Epoch 88/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3476 - accuracy: 0.8721\n",
      "Epoch 89/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3473 - accuracy: 0.8725\n",
      "Epoch 90/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3466 - accuracy: 0.8723\n",
      "Epoch 91/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3462 - accuracy: 0.8723\n",
      "Epoch 92/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3457 - accuracy: 0.8727\n",
      "Epoch 93/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3448 - accuracy: 0.8728\n",
      "Epoch 94/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3443 - accuracy: 0.8728\n",
      "Epoch 95/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3441 - accuracy: 0.8722\n",
      "Epoch 96/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3434 - accuracy: 0.8731\n",
      "Epoch 97/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3425 - accuracy: 0.8729\n",
      "Epoch 98/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3425 - accuracy: 0.8730\n",
      "Epoch 99/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3416 - accuracy: 0.8731\n",
      "Epoch 100/100\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.3410 - accuracy: 0.8733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe223ef0c40>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=epochs, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e7feae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72582ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame(index=np.append(tasks, ['Macro', 'Micro']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7265176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# calculate AUC for every cohort\n",
    "lst_of_auc = []\n",
    "for task in tasks:\n",
    "    auc = roc_auc_score(y_test[cohorts_test == task], y_pred[cohorts_test == task])\n",
    "    lst_of_auc.append(auc)\n",
    "    df_metrics.loc[task, 'AUC'] = auc\n",
    "\n",
    "# calculate macro AUC\n",
    "df_metrics.loc['Macro', 'AUC'] = np.nanmean(np.array(lst_of_auc), axis=0)\n",
    "\n",
    "# calculate micro AUC\n",
    "df_metrics.loc['Micro', 'AUC'] = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfbbdac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CCU</th>\n",
       "      <td>0.509169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSRU</th>\n",
       "      <td>0.517299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MICU</th>\n",
       "      <td>0.521928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SICU</th>\n",
       "      <td>0.489976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSICU</th>\n",
       "      <td>0.451812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Macro</th>\n",
       "      <td>0.498037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Micro</th>\n",
       "      <td>0.506323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AUC\n",
       "CCU    0.509169\n",
       "CSRU   0.517299\n",
       "MICU   0.521928\n",
       "SICU   0.489976\n",
       "TSICU  0.451812\n",
       "Macro  0.498037\n",
       "Micro  0.506323"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15050d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5063227718037842"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred, average='micro', multi_class='ovr', labels=cohorts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfe75665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4980368353047825"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(np.array(lst_of_auc), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d73b3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5063227718037842"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred, average='macro', multi_class='ovr', labels=cohorts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b00f8011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[cohorts_test == tasks[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef3a2086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[cohorts_test == tasks[0]] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc47c070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
